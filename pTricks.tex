\documentclass{article}

\section{Probability Cheat Sheet}

This was originally going to be a part of the Probability post, but it just got way too long. We were allowed one sheet of paper, double sided, for the exam, and this is what I chose to write on mine.

\subsection{Major Results (that you're going to forget)}

 \textbf{Borel-Cantelli}
 If $\sum\Bbb P(A_n)$ is finite, then $\Bbb P(\bigcap_{m=1^\infty}\bigcup_{n=m}^\infty A_n)=0$.

 \textbf{Large Numbers} 
If $(X_n)$ is pairwise independent, identically distributed and has a finite mean, then $\frac1n(X_1+X_2+\cdots+X_n)\to\Bbb E(X_1)$ almost surely.

\textbf{Central Limit} 
If $(X_n)$ are i.i.d. with finite mean and nonzero finite variance, then $\frac{\sqrt{n}}{\sigma}\left(\frac1n(X_1+X_2+\cdots+X_n)-\mu\right)$ converges (as $n\to\infty$) in distribution to the standard normal $N(0,1)$.

\textbf{Kolmogorov's Zero-One} 
If $(X_n)$ are independent and $\tau=\bigcap_{n=1}^\infty \sigma(X_n, X_{n+1},\dots)$, then every event $A\in\tau$ has either $\Bbb P(A)=0$ or $\Bbb P(A)=1$.

\textbf{Lindeberg-Feller}
For each $n$, let $\\{X_{n,k}\\}_{k\leq n}$ be independent. If $\Bbb E(X_{n,k})=0$ and $\sum_k \Bbb E({X_{n,k}}^2)$ converges to a finite value $\sigma^2$. and for each $\varepsilon>0$ the sum $\sum_k \Bbb E({X_{n,k}}^21_{|X_{n,k}|>\varepsilon})$ converges to zero, then $X_{n,1}+X_{n,2}+\cdots+X_{n,n}$ converges in distribution to $N(0,\sigma^2)$.

\textbf{Dynkin $\pi-\lambda$}
If $\mathcal A$ is a collection of subsets closed under intersections containing $\varnothing$, and $\mathcal B$ is a collection containing $\Omega$, closed under chain unions, and closed under chain complements. then $\mathcal A\subseteq\mathcal B$ implies that $\sigma(\mathcal A)\subseteq\mathcal B$.

\textbf{Markov}
 $\Bbb P(|X|\geq t)\leq \frac1t\Bbb E|X|$.

\textbf{Chebychev}
$\Bbb P(|X-\Bbb E X|\geq t) \leq \frac{1}{t^2}\Bbb V(X)$.
\textbf{Kolmogorov}
If $(X_i)$ are independent with mean zero and finite variance, then $$\Bbb P(\max_{1\leq i\leq n} |X_1+X_2+\cdots|>x) \leq \frac{\Bbb V(X_1+X_2+\cdots + X_n)}{x^2}.$$

\subsection{Useful Tricks}

Prove a statement $\Phi( C)$ for all $C\in\sigma(\mathcal F, \mathcal G)$, if true for all $A\in\mathcal F$ and $B\in\mathcal G$, by setting $\mathcal C=\\{C\in\sigma(\mathcal F, \mathcal G): \Phi(C)\\}$ and trying to use $\pi-\lambda$.


If $\sigma(X)$ is independent of $\mathcal F$, then for $A\in\mathcal F$ we have that $\int X1_A = \int X\cdot\int 1_A$.

If $X$ and $Y$ have the same distribution, then $\Bbb E f(X)= \Bbb Ef(Y)$ for $f$ with $\Bbb E|f(X)|<\infty$. 

Also, $(X,Y)$ and $(Y,X)$ have the same distribution, so you can use a nonsymmetric 2-variable function to generate symmetric results.


A problem in HW says that [Radon-Nikodym] holds in the weighted sense as well: If $g$ is measurable then $\int g\, d\mu = \int g \frac{d\mu}{d\nu}\,d\nu$.


If you can prove something for $X$ with some number of finite moments, then you may be able to prove it for more general $X$ by decomposing $X=X1_{X>a}+X1_{X\leq a}$ for a carefully chosen $a$; the first has good moment properties, the second might be small in some sense.


Using Helly's Selection Theorem: First, prove tightness of the sequence. HST guarantees the existence of some subsequential limit, prove a uniqueness result functions of the relevant form, and then apply that result to show that any two subsequential limits must be the same, so HST is the liminf and limsup.

 The smoothness at $\varphi$ at zero corresponds to the rate of decay of $\mu$ at $\infty$. The Fourier differentiation rule is a weak formal version of this: If $\Bbb E X^k<\infty$, then $\phi_X^{(k)}(t) = \Bbb E[(iX)^ke^{itX}]$.

Something that I didn't have on the paper, was the defintion of a tight sequence of measures. 


Tightness (probability measures): For all $\varepsilon$ and all $n$, there exists compact $K$ such that $\mu_n(K)>1-\varepsilon$.

This was very silly, since I both knew that I didn't know it and also that the professor thought this was one of the more important topics in the course. 


\end{document}